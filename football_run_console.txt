      (0): norm_data(
        (bn): BatchNorm1d(75, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): cnn1x1(
        (cnn): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1))
      )
      (2): ReLU()
      (3): cnn1x1(
        (cnn): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
      )
      (4): ReLU()
    )
  )
  (maxpool): AdaptiveMaxPool2d(output_size=(1, 1))
  (cnn): local(
    (maxpool): AdaptiveMaxPool2d(output_size=(1, 20))
    (cnn1): Conv2d(256, 256, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU()
    (cnn2): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (dropout): Dropout2d(p=0.2, inplace=False)
  )
  (compute_g1): compute_g_spa(
    (g1): cnn1x1(
      (cnn): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
    )
    (g2): cnn1x1(
      (cnn): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
    )
    (softmax): Softmax(dim=-1)
  )
  (gcn1): gcn_spa(
    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU()
    (w): cnn1x1(
      (cnn): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (w1): cnn1x1(
      (cnn): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (gcn2): gcn_spa(
    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU()
    (w): cnn1x1(
      (cnn): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (w1): cnn1x1(
      (cnn): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (gcn3): gcn_spa(
    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU()
    (w): cnn1x1(
      (cnn): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (w1): cnn1x1(
      (cnn): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (fc): Linear(in_features=512, out_features=2, bias=True)
)
The number of parameters:  661550
The modes is: SGN
It is using GPU!
Train on 1369 samples, validate on 343 samples
0 0.001
Backend qtagg is interactive backend. Turning interactive mode on.
Backend qtagg is interactive backend. Turning interactive mode on.
Backend qtagg is interactive backend. Turning interactive mode on.
Backend qtagg is interactive backend. Turning interactive mode on.
Backend qtagg is interactive backend. Turning interactive mode on.
Backend qtagg is interactive backend. Turning interactive mode on.
Backend qtagg is interactive backend. Turning interactive mode on.
Backend qtagg is interactive backend. Turning interactive mode on.
Backend qtagg is interactive backend. Turning interactive mode on.
Backend qtagg is interactive backend. Turning interactive mode on.
Backend qtagg is interactive backend. Turning interactive mode on.
Backend qtagg is interactive backend. Turning interactive mode on.
Backend qtagg is interactive backend. Turning interactive mode on.
Backend qtagg is interactive backend. Turning interactive mode on.
Backend qtagg is interactive backend. Turning interactive mode on.
Backend qtagg is interactive backend. Turning interactive mode on.
Backend qtagg is interactive backend. Turning interactive mode on.
(base) louiemiddle@HEI-WINalNumber:~/SGN$ ^C

(base) louiemiddle@HEI-WINalNumber:~/SGN$  cd /home/louiemiddle/SGN ; /usr/bin/env /home/louiemiddle/anaconda3/bin/python /home/louiemiddle/.vscode-server/extensions/ms-python.debugpy-2024.14.0-linux-x64/bundled/libs/debugpy/adapter/../../debugpy/launcher 34943 -- /home/louiemiddle/SGN/main.py 
SGN(
  (tem_embed): embed(
    (cnn): Sequential(
      (0): cnn1x1(
        (cnn): Conv2d(20, 64, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): ReLU()
      (2): cnn1x1(
        (cnn): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (3): ReLU()
    )
  )
  (spa_embed): embed(
    (cnn): Sequential(
      (0): cnn1x1(
        (cnn): Conv2d(29, 64, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): ReLU()
      (2): cnn1x1(
        (cnn): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
      )
      (3): ReLU()
    )
  )
  (joint_embed): embed(
    (cnn): Sequential(
      (0): norm_data(
        (bn): BatchNorm1d(75, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): cnn1x1(
        (cnn): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1))
      )
      (2): ReLU()
      (3): cnn1x1(
        (cnn): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
      )
      (4): ReLU()
    )
  )
  (dif_embed): embed(
    (cnn): Sequential(
      (0): norm_data(
        (bn): BatchNorm1d(75, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): cnn1x1(
        (cnn): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1))
      )
      (2): ReLU()
      (3): cnn1x1(
        (cnn): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
      )
      (4): ReLU()
    )
  )
  (maxpool): AdaptiveMaxPool2d(output_size=(1, 1))
  (cnn): local(
    (maxpool): AdaptiveMaxPool2d(output_size=(1, 20))
    (cnn1): Conv2d(256, 256, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU()
    (cnn2): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (dropout): Dropout2d(p=0.2, inplace=False)
  )
  (compute_g1): compute_g_spa(
    (g1): cnn1x1(
      (cnn): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
    )
    (g2): cnn1x1(
      (cnn): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
    )
    (softmax): Softmax(dim=-1)
  )
  (gcn1): gcn_spa(
    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU()
    (w): cnn1x1(
      (cnn): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (w1): cnn1x1(
      (cnn): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (gcn2): gcn_spa(
    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU()
    (w): cnn1x1(
      (cnn): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (w1): cnn1x1(
      (cnn): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (gcn3): gcn_spa(
    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU()
    (w): cnn1x1(
      (cnn): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (w1): cnn1x1(
      (cnn): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (fc): Linear(in_features=512, out_features=2, bias=True)
)
The number of parameters:  661550
The modes is: SGN
It is using GPU!
Train on 1369 samples, validate on 343 samples
0 0.001
Backend qtagg is interactive backend. Turning interactive mode on.
(base) louiemiddle@HEI-WINalNumber:~/SGN$ ^C

(base) louiemiddle@HEI-WINalNumber:~/SGN$  cd /home/louiemiddle/SGN ; /usr/bin/env /home/louiemiddle/anaconda3/bin/python /home/louiemiddle/.vscode-server/extensions/ms-python.debugpy-2024.14.0-linux-x64/bundled/libs/debugpy/adapter/../../debugpy/launcher 35639 -- /home/louiemiddle/SGN/main.py 
SGN(
  (tem_embed): embed(
    (cnn): Sequential(
      (0): cnn1x1(
        (cnn): Conv2d(20, 64, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): ReLU()
      (2): cnn1x1(
        (cnn): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (3): ReLU()
    )
  )
  (spa_embed): embed(
    (cnn): Sequential(
      (0): cnn1x1(
        (cnn): Conv2d(29, 64, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): ReLU()
      (2): cnn1x1(
        (cnn): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
      )
      (3): ReLU()
    )
  )
  (joint_embed): embed(
    (cnn): Sequential(
      (0): norm_data(
        (bn): BatchNorm1d(87, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): cnn1x1(
        (cnn): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1))
      )
      (2): ReLU()
      (3): cnn1x1(
        (cnn): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
      )
      (4): ReLU()
    )
  )
  (dif_embed): embed(
    (cnn): Sequential(
      (0): norm_data(
        (bn): BatchNorm1d(87, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): cnn1x1(
        (cnn): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1))
      )
      (2): ReLU()
      (3): cnn1x1(
        (cnn): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
      )
      (4): ReLU()
    )
  )
  (maxpool): AdaptiveMaxPool2d(output_size=(1, 1))
  (cnn): local(
    (maxpool): AdaptiveMaxPool2d(output_size=(1, 20))
    (cnn1): Conv2d(256, 256, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU()
    (cnn2): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (dropout): Dropout2d(p=0.2, inplace=False)
  )
  (compute_g1): compute_g_spa(
    (g1): cnn1x1(
      (cnn): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
    )
    (g2): cnn1x1(
      (cnn): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
    )
    (softmax): Softmax(dim=-1)
  )
  (gcn1): gcn_spa(
    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU()
    (w): cnn1x1(
      (cnn): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (w1): cnn1x1(
      (cnn): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (gcn2): gcn_spa(
    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU()
    (w): cnn1x1(
      (cnn): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (w1): cnn1x1(
      (cnn): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (gcn3): gcn_spa(
    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU()
    (w): cnn1x1(
      (cnn): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (w1): cnn1x1(
      (cnn): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (fc): Linear(in_features=512, out_features=2, bias=True)
)
The number of parameters:  661598
The modes is: SGN
It is using GPU!
Train on 1369 samples, validate on 343 samples
0 0.001
Backend qtagg is interactive backend. Turning interactive mode on.
(base) louiemiddle@HEI-WINalNumber:~/SGN$ ^C

(base) louiemiddle@HEI-WINalNumber:~/SGN$  cd /home/louiemiddle/SGN ; /usr/bin/env /home/louiemiddle/anaconda3/bin/python /home/louiemiddle/.vscode-server/extensions/ms-python.debugpy-2024.14.0-linux-x64/bundled/libs/debugpy/adapter/../../debugpy/launcher 47337 -- /home/louiemiddle/SGN/main.py 
SGN(
  (tem_embed): embed(
    (cnn): Sequential(
      (0): cnn1x1(
        (cnn): Conv2d(20, 64, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): ReLU()
      (2): cnn1x1(
        (cnn): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (3): ReLU()
    )
  )
  (spa_embed): embed(
    (cnn): Sequential(
      (0): cnn1x1(
        (cnn): Conv2d(29, 64, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): ReLU()
      (2): cnn1x1(
        (cnn): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
      )
      (3): ReLU()
    )
  )
  (joint_embed): embed(
    (cnn): Sequential(
      (0): norm_data(
        (bn): BatchNorm1d(87, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): cnn1x1(
        (cnn): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1))
      )
      (2): ReLU()
      (3): cnn1x1(
        (cnn): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
      )
      (4): ReLU()
    )
  )
  (dif_embed): embed(
    (cnn): Sequential(
      (0): norm_data(
        (bn): BatchNorm1d(87, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): cnn1x1(
        (cnn): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1))
      )
      (2): ReLU()
      (3): cnn1x1(
        (cnn): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
      )
      (4): ReLU()
    )
  )
  (maxpool): AdaptiveMaxPool2d(output_size=(1, 1))
  (cnn): local(
    (maxpool): AdaptiveMaxPool2d(output_size=(1, 20))
    (cnn1): Conv2d(256, 256, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU()
    (cnn2): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (dropout): Dropout2d(p=0.2, inplace=False)
  )
  (compute_g1): compute_g_spa(
    (g1): cnn1x1(
      (cnn): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
    )
    (g2): cnn1x1(
      (cnn): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
    )
    (softmax): Softmax(dim=-1)
  )
  (gcn1): gcn_spa(
    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU()
    (w): cnn1x1(
      (cnn): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (w1): cnn1x1(
      (cnn): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (gcn2): gcn_spa(
    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU()
    (w): cnn1x1(
      (cnn): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (w1): cnn1x1(
      (cnn): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
    )
  )
(base) louiemiddle@HEI-WINalNumber:~/SGN$ ^C

(base) louiemiddle@HEI-WINalNumber:~/SGN$  cd /home/louiemiddle/SGN ; /usr/bin/env /home/louiemiddle/anaconda3/bin/python /home/louiemiddle/.vscode-server/extensions/ms-python.debugpy-2024.14.0-linux-x64/bundled/libs/debugpy/adapter/../../debugpy/launcher 36925 -- /home/louiemiddle/SGN/main.py 
SGN(
  (tem_embed): embed(
    (cnn): Sequential(
      (0): cnn1x1(
        (cnn): Conv2d(20, 64, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): ReLU()
      (2): cnn1x1(
        (cnn): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (3): ReLU()
    )
  )
  (spa_embed): embed(
    (cnn): Sequential(
      (0): cnn1x1(
        (cnn): Conv2d(29, 64, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): ReLU()
      (2): cnn1x1(
        (cnn): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
      )
      (3): ReLU()
    )
  )
  (joint_embed): embed(
    (cnn): Sequential(
      (0): norm_data(
        (bn): BatchNorm1d(87, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): cnn1x1(
        (cnn): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1))
      )
      (2): ReLU()
      (3): cnn1x1(
        (cnn): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
      )
      (4): ReLU()
    )
  )
  (dif_embed): embed(
    (cnn): Sequential(
      (0): norm_data(
        (bn): BatchNorm1d(87, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): cnn1x1(
        (cnn): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1))
      )
      (2): ReLU()
      (3): cnn1x1(
        (cnn): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
      )
      (4): ReLU()
    )
  )
  (maxpool): AdaptiveMaxPool2d(output_size=(1, 1))
  (cnn): local(
    (maxpool): AdaptiveMaxPool2d(output_size=(1, 20))
    (cnn1): Conv2d(256, 256, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU()
    (cnn2): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (dropout): Dropout2d(p=0.2, inplace=False)
  )
  (compute_g1): compute_g_spa(
    (g1): cnn1x1(
      (cnn): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
    )
    (g2): cnn1x1(
      (cnn): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
    )
    (softmax): Softmax(dim=-1)
  )
  (gcn1): gcn_spa(
    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU()
    (w): cnn1x1(
      (cnn): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (w1): cnn1x1(
      (cnn): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (gcn2): gcn_spa(
    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU()
    (w): cnn1x1(
      (cnn): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (w1): cnn1x1(
      (cnn): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (gcn3): gcn_spa(
    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU()
    (w): cnn1x1(
      (cnn): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (w1): cnn1x1(
      (cnn): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (fc): Linear(in_features=512, out_features=2, bias=True)
)
The number of parameters:  661598
The modes is: SGN
It is using GPU!
Train on 1369 samples, validate on 343 samples
0 0.001
Epoch-1    20 batches   loss 0.7891 (0.8434)    accu 68.750 (68.828)
Epoch-1   15.3s Train: loss 0.8352      accu 68.7500    Valid: loss 0.6136      accu 70.0000
Epoch 1: val_acc improved from -inf to 70.0000, saving model to ./results/Football/SGN/0_best.pth
1 0.001
Epoch-2    20 batches   loss 0.8187 (0.7057)    accu 64.062 (70.469)
Epoch-2   15.4s Train: loss 0.7103      accu 70.1637    Valid: loss 0.7045      accu 69.3750
Epoch 2: val_acc did not improve
2 0.001
Epoch-3    20 batches   loss 0.6809 (0.6770)    accu 71.875 (70.703)
Epoch-3   14.8s Train: loss 0.6750      accu 70.9077    Valid: loss 0.5922      accu 75.9375
Epoch 3: val_acc improved from 70.0000 to 75.9375, saving model to ./results/Football/SGN/0_best.pth
3 0.001
Epoch-4    20 batches   loss 0.6116 (0.6527)    accu 73.438 (71.328)
Epoch-4   14.8s Train: loss 0.6519      accu 71.3542    Valid: loss 0.6021      accu 75.3125
Epoch 4: val_acc did not improve
4 0.001
Epoch-5    20 batches   loss 0.6816 (0.6550)    accu 71.875 (70.625)
Epoch-5   15.7s Train: loss 0.6531      accu 71.2054    Valid: loss 0.6101      accu 75.6250
Epoch 5: val_acc did not improve
5 0.001
Epoch-6    20 batches   loss 0.6069 (0.6343)    accu 71.875 (71.953)
Epoch-6   14.8s Train: loss 0.6353      accu 72.0238    Valid: loss 0.6009      accu 75.3125
Epoch 6: val_acc did not improve
6 0.001
Epoch-7    20 batches   loss 0.5937 (0.6278)    accu 75.000 (72.578)
Epoch-7   15.1s Train: loss 0.6303      accu 72.3958    Valid: loss 0.5956      accu 73.4375
Epoch 7: val_acc did not improve
7 0.001
Epoch-8    20 batches   loss 0.6175 (0.6385)    accu 78.125 (72.109)
Epoch-8   15.7s Train: loss 0.6357      accu 72.5446    Valid: loss 0.5956      accu 75.3125
Epoch 8: val_acc did not improve
8 0.001
Epoch-9    20 batches   loss 0.6141 (0.6278)    accu 78.125 (72.188)
Epoch-9   15.0s Train: loss 0.6266      accu 72.1726    Valid: loss 0.6097      accu 73.4375
Epoch 9: val_acc did not improve
9 0.001
Epoch-10   20 batches   loss 0.5511 (0.6257)    accu 81.250 (72.109)
Epoch-10  15.1s Train: loss 0.6290      accu 71.5774    Valid: loss 0.5967      accu 76.2500
Epoch 10: val_acc improved from 75.9375 to 76.2500, saving model to ./results/Football/SGN/0_best.pth
10 0.001
Epoch-11   20 batches   loss 0.5677 (0.6209)    accu 84.375 (73.203)
Epoch-11  15.3s Train: loss 0.6200      accu 73.1399    Valid: loss 0.5967      accu 75.0000
Epoch 11: val_acc did not improve
11 0.001
Epoch-12   20 batches   loss 0.6446 (0.6233)    accu 68.750 (72.344)
Epoch-12  14.6s Train: loss 0.6216      accu 72.7679    Valid: loss 0.6045      accu 74.3750
Epoch 12: val_acc did not improve
12 0.001
Epoch-13   20 batches   loss 0.6280 (0.6253)    accu 75.000 (72.500)
Epoch-13  15.1s Train: loss 0.6262      accu 72.3958    Valid: loss 0.6015      accu 75.3125
Epoch 13: val_acc did not improve
13 0.001
Epoch-14   20 batches   loss 0.6437 (0.6221)    accu 68.750 (72.188)
Epoch-14  15.0s Train: loss 0.6224      accu 72.0982    Valid: loss 0.5984      accu 76.2500
Epoch 14: val_acc did not improve
14 0.001
Epoch-15   20 batches   loss 0.6317 (0.6233)    accu 68.750 (73.125)
Epoch-15  15.8s Train: loss 0.6215      accu 73.2887    Valid: loss 0.6172      accu 71.5625
Epoch 15: val_acc did not improve
15 0.001
Epoch-16   20 batches   loss 0.5910 (0.6315)    accu 81.250 (72.656)
Epoch-16  14.9s Train: loss 0.6319      accu 72.3958    Valid: loss 0.6011      accu 76.2500
Epoch 16: val_acc did not improve
16 0.001
Epoch-17   20 batches   loss 0.7053 (0.6191)    accu 65.625 (73.750)
Epoch-17  15.3s Train: loss 0.6185      accu 73.8095    Valid: loss 0.5881      accu 76.2500
Epoch 17: val_acc did not improve
17 0.001
Epoch-18   20 batches   loss 0.5729 (0.5847)    accu 78.125 (75.547)
Epoch-18  15.3s Train: loss 0.5890      accu 75.2232    Valid: loss 0.5622      accu 79.0625
Epoch 18: val_acc improved from 76.2500 to 79.0625, saving model to ./results/Football/SGN/0_best.pth
18 0.001
Epoch-19   20 batches   loss 0.5994 (0.5981)    accu 75.000 (75.078)
Epoch-19  14.8s Train: loss 0.5959      accu 75.2232    Valid: loss 0.5652      accu 78.7500
Epoch 19: val_acc did not improve
19 0.001
Epoch-20   20 batches   loss 0.7084 (0.6011)    accu 67.188 (75.156)
Epoch-20  15.2s Train: loss 0.6014      accu 75.2976    Valid: loss 0.6167      accu 75.9375
Epoch 20: val_acc did not improve
20 0.001
Epoch-21   20 batches   loss 0.5534 (0.5940)    accu 81.250 (76.641)
Epoch-21  15.6s Train: loss 0.5921      accu 76.7113    Valid: loss 0.5502      accu 79.3750
Epoch 21: val_acc improved from 79.0625 to 79.3750, saving model to ./results/Football/SGN/0_best.pth
21 0.001
Epoch-22   20 batches   loss 0.5511 (0.5737)    accu 82.812 (76.953)
Epoch-22  15.1s Train: loss 0.5746      accu 76.6369    Valid: loss 0.5517      accu 76.2500
Epoch 22: val_acc did not improve
22 0.001
Epoch-23   20 batches   loss 0.6065 (0.5695)    accu 71.875 (77.422)
Epoch-23  15.1s Train: loss 0.5698      accu 77.4554    Valid: loss 0.5462      accu 80.0000
Epoch 23: val_acc improved from 79.3750 to 80.0000, saving model to ./results/Football/SGN/0_best.pth
23 0.001
Epoch-24   20 batches   loss 0.6043 (0.5788)    accu 73.438 (77.266)
Epoch-24  15.5s Train: loss 0.5782      accu 77.3810    Valid: loss 0.5457      accu 79.3750
Epoch 24: val_acc did not improve
24 0.001
Epoch-25   20 batches   loss 0.5506 (0.5635)    accu 76.562 (77.734)
Epoch-25  14.7s Train: loss 0.5638      accu 77.5298    Valid: loss 0.5342      accu 80.0000
Epoch 25: val_acc did not improve
25 0.001
Epoch-26   20 batches   loss 0.6747 (0.5682)    accu 67.188 (77.500)
Epoch-26  15.5s Train: loss 0.5712      accu 77.1577    Valid: loss 0.5593      accu 76.8750
Epoch 26: val_acc did not improve
26 0.001
Epoch-27   20 batches   loss 0.5159 (0.5619)    accu 85.938 (77.656)
Epoch-27  14.9s Train: loss 0.5617      accu 77.6042    Valid: loss 0.5441      accu 80.3125
Epoch 27: val_acc improved from 80.0000 to 80.3125, saving model to ./results/Football/SGN/0_best.pth
27 0.001
Epoch-28   20 batches   loss 0.6156 (0.5613)    accu 71.875 (77.734)
Epoch-28  15.1s Train: loss 0.5607      accu 77.6042    Valid: loss 0.5653      accu 78.1250
Epoch 28: val_acc did not improve
28 0.001
Epoch-29   20 batches   loss 0.5869 (0.5530)    accu 73.438 (79.922)
Epoch-29  14.9s Train: loss 0.5550      accu 79.8363    Valid: loss 0.5358      accu 80.3125
Epoch 29: val_acc did not improve
29 0.001
Epoch-30   20 batches   loss 0.5492 (0.5504)    accu 79.688 (78.828)
Epoch-30  15.1s Train: loss 0.5537      accu 78.4970    Valid: loss 0.5259      accu 80.3125
Epoch 30: val_acc did not improve
30 0.001
Epoch-31   20 batches   loss 0.5984 (0.5404)    accu 73.438 (79.844)
Epoch-31  15.5s Train: loss 0.5413      accu 79.9107    Valid: loss 0.5224      accu 81.5625
Epoch 31: val_acc improved from 80.3125 to 81.5625, saving model to ./results/Football/SGN/0_best.pth
31 0.001
Epoch-32   20 batches   loss 0.5194 (0.5419)    accu 78.125 (79.062)
Epoch-32  15.3s Train: loss 0.5400      accu 79.3899    Valid: loss 0.5229      accu 80.6250
Epoch 32: val_acc did not improve
32 0.001
Epoch-33   20 batches   loss 0.6234 (0.5576)    accu 73.438 (78.203)
Epoch-33  14.9s Train: loss 0.5530      accu 78.6458    Valid: loss 0.5147      accu 81.8750
Epoch 33: val_acc improved from 81.5625 to 81.8750, saving model to ./results/Football/SGN/0_best.pth
33 0.001
Epoch-34   20 batches   loss 0.5305 (0.5463)    accu 75.000 (78.906)
Epoch-34  14.8s Train: loss 0.5433      accu 79.4643    Valid: loss 0.5186      accu 81.8750
Epoch 34: val_acc did not improve
34 0.001
Epoch-35   20 batches   loss 0.6157 (0.5384)    accu 68.750 (80.391)
Epoch-35  15.5s Train: loss 0.5370      accu 80.2083    Valid: loss 0.5064      accu 85.0000
Epoch 35: val_acc improved from 81.8750 to 85.0000, saving model to ./results/Football/SGN/0_best.pth
35 0.001
Epoch-36   20 batches   loss 0.4946 (0.5371)    accu 84.375 (81.328)
Epoch-36  14.4s Train: loss 0.5409      accu 81.0268    Valid: loss 0.5094      accu 84.0625
Epoch 36: val_acc did not improve
36 0.001
Epoch-37   20 batches   loss 0.5665 (0.5287)    accu 75.000 (81.406)
Epoch-37  15.7s Train: loss 0.5266      accu 81.3988    Valid: loss 0.5100      accu 84.0625
Epoch 37: val_acc did not improve
37 0.001
Epoch-38   20 batches   loss 0.5464 (0.5158)    accu 82.812 (82.500)
Epoch-38  14.9s Train: loss 0.5151      accu 82.5893    Valid: loss 0.5078      accu 85.3125
Epoch 38: val_acc improved from 85.0000 to 85.3125, saving model to ./results/Football/SGN/0_best.pth
38 0.001
Epoch-39   20 batches   loss 0.5511 (0.5324)    accu 79.688 (80.625)
Epoch-39  15.1s Train: loss 0.5290      accu 80.9524    Valid: loss 0.5119      accu 85.0000
Epoch 39: val_acc did not improve
39 0.001
Epoch-40   20 batches   loss 0.4968 (0.5212)    accu 84.375 (81.797)
Epoch-40  14.3s Train: loss 0.5180      accu 81.9940    Valid: loss 0.5032      accu 85.0000
Epoch 40: val_acc did not improve
40 0.001
Epoch-41   20 batches   loss 0.5815 (0.5180)    accu 71.875 (82.812)
Epoch-41  15.4s Train: loss 0.5217      accu 82.5149    Valid: loss 0.5161      accu 82.5000
Epoch 41: val_acc did not improve
41 0.001
Epoch-42   20 batches   loss 0.5202 (0.5162)    accu 84.375 (83.203)
Epoch-42  14.9s Train: loss 0.5144      accu 83.3333    Valid: loss 0.5199      accu 83.1250
Epoch 42: val_acc did not improve
42 0.001
Epoch-43   20 batches   loss 0.5402 (0.5274)    accu 78.125 (81.875)
Epoch-43  15.2s Train: loss 0.5259      accu 82.0685    Valid: loss 0.5096      accu 84.6875
Epoch 43: val_acc did not improve
43 0.001
Epoch-44   20 batches   loss 0.5248 (0.5189)    accu 85.938 (82.266)
Epoch-44  15.7s Train: loss 0.5179      accu 82.3661    Valid: loss 0.5044      accu 85.0000
Epoch 44: val_acc did not improve
44 0.001
Epoch-45   20 batches   loss 0.5348 (0.5229)    accu 76.562 (81.875)
Epoch-45  15.3s Train: loss 0.5239      accu 81.6964    Valid: loss 0.5015      accu 84.6875
Epoch 45: val_acc did not improve
45 0.001
Epoch-46   20 batches   loss 0.4825 (0.5010)    accu 82.812 (84.219)
Epoch-46  14.9s Train: loss 0.5032      accu 83.9286    Valid: loss 0.5085      accu 85.6250
Epoch 46: val_acc improved from 85.3125 to 85.6250, saving model to ./results/Football/SGN/0_best.pth
46 0.001
Epoch-47   20 batches   loss 0.5208 (0.5000)    accu 81.250 (84.688)
Epoch-47  15.4s Train: loss 0.4985      accu 84.8958    Valid: loss 0.4945      accu 84.6875
Epoch 47: val_acc did not improve
47 0.001
Epoch-48   20 batches   loss 0.4673 (0.5180)    accu 89.062 (83.281)
Epoch-48  15.3s Train: loss 0.5182      accu 83.4077    Valid: loss 0.5059      accu 83.7500
Epoch 48: val_acc did not improve
48 0.001
Epoch-49   20 batches   loss 0.5162 (0.4979)    accu 81.250 (84.375)
Epoch-49  14.9s Train: loss 0.5000      accu 84.0774    Valid: loss 0.5384      accu 82.5000
Epoch 49: val_acc did not improve
49 0.001
Epoch-50   20 batches   loss 0.4746 (0.4892)    accu 85.938 (85.859)
Epoch-50  14.9s Train: loss 0.4920      accu 85.7143    Valid: loss 0.5172      accu 84.0625
Epoch 50: val_acc did not improve
50 0.001
Epoch-51   20 batches   loss 0.4906 (0.4957)    accu 87.500 (84.922)
Epoch-51  15.1s Train: loss 0.4962      accu 84.9702    Valid: loss 0.5013      accu 85.0000
Epoch 51: val_acc did not improve
51 0.001
Epoch-52   20 batches   loss 0.4697 (0.4887)    accu 82.812 (85.547)
Epoch-52  14.8s Train: loss 0.4872      accu 85.7143    Valid: loss 0.5105      accu 85.3125
Epoch 52: val_acc did not improve
52 0.001
Epoch-53   20 batches   loss 0.4810 (0.4970)    accu 85.938 (85.156)
Epoch-53  14.9s Train: loss 0.4953      accu 85.4911    Valid: loss 0.5017      accu 85.9375
Epoch 53: val_acc improved from 85.6250 to 85.9375, saving model to ./results/Football/SGN/0_best.pth
53 0.001
Epoch-54   20 batches   loss 0.4854 (0.4813)    accu 85.938 (87.109)
Epoch-54  15.2s Train: loss 0.4808      accu 87.1280    Valid: loss 0.5047      accu 86.2500
Epoch 54: val_acc improved from 85.9375 to 86.2500, saving model to ./results/Football/SGN/0_best.pth
54 0.001
Epoch-55   20 batches   loss 0.4526 (0.4704)    accu 90.625 (87.578)
Epoch-55  15.1s Train: loss 0.4759      accu 87.2024    Valid: loss 0.5134      accu 84.6875
Epoch 55: val_acc did not improve
55 0.001
Epoch-56   20 batches   loss 0.4779 (0.4794)    accu 87.500 (86.562)
Epoch-56  15.1s Train: loss 0.4790      accu 86.7560    Valid: loss 0.5367      accu 83.7500
Epoch 56: val_acc did not improve
56 0.001
Epoch-57   20 batches   loss 0.4806 (0.4879)    accu 87.500 (86.016)
Epoch-57  14.8s Train: loss 0.4884      accu 85.8631    Valid: loss 0.5052      accu 85.0000
Epoch 57: val_acc did not improve
57 0.001
Epoch-58   20 batches   loss 0.4648 (0.4619)    accu 89.062 (88.359)
Epoch-58  15.0s Train: loss 0.4635      accu 88.1696    Valid: loss 0.5082      accu 85.0000
Epoch 58: val_acc did not improve
58 0.001
Epoch-59   20 batches   loss 0.4291 (0.4631)    accu 89.062 (87.891)
Epoch-59  15.6s Train: loss 0.4641      accu 87.7976    Valid: loss 0.4993      accu 87.1875
Epoch 59: val_acc improved from 86.2500 to 87.1875, saving model to ./results/Football/SGN/0_best.pth
59 0.001
Epoch-60   20 batches   loss 0.4467 (0.4786)    accu 87.500 (87.188)
Epoch-60  15.0s Train: loss 0.4790      accu 87.2768    Valid: loss 0.5134      accu 83.4375
Epoch 60: val_acc did not improve
60 0.0001
Epoch-61   20 batches   loss 0.4530 (0.4478)    accu 85.938 (89.219)
Epoch-61  15.4s Train: loss 0.4527      accu 88.7649    Valid: loss 0.5047      accu 84.3750
Epoch 61: val_acc did not improve
61 0.0001
Epoch-62   20 batches   loss 0.4384 (0.4524)    accu 89.062 (89.219)
Epoch-62  15.1s Train: loss 0.4521      accu 89.2857    Valid: loss 0.5006      accu 85.0000
Epoch 62: val_acc did not improve
62 0.0001
Epoch-63   20 batches   loss 0.4500 (0.4535)    accu 90.625 (89.531)
Epoch-63  14.3s Train: loss 0.4527      accu 89.5833    Valid: loss 0.5066      accu 85.3125
Epoch 63: val_acc did not improve
63 0.0001
Epoch-64   20 batches   loss 0.5247 (0.4480)    accu 78.125 (89.844)
Epoch-64  15.4s Train: loss 0.4473      accu 89.8065    Valid: loss 0.4970      accu 85.3125
Epoch 64: val_acc did not improve
64 0.0001
Epoch-65   20 batches   loss 0.4527 (0.4458)    accu 89.062 (89.922)
Epoch-65  15.0s Train: loss 0.4431      accu 90.2530    Valid: loss 0.4961      accu 85.3125
Epoch 65: val_acc did not improve
65 0.0001
Epoch-66   20 batches   loss 0.4495 (0.4432)    accu 93.750 (90.625)
Epoch-66  14.5s Train: loss 0.4435      accu 90.6250    Valid: loss 0.4942      accu 86.2500
Epoch 66: val_acc did not improve
66 0.0001
Epoch-67   20 batches   loss 0.4503 (0.4475)    accu 87.500 (89.531)
Epoch-67  15.1s Train: loss 0.4491      accu 89.5089    Valid: loss 0.4986      accu 85.0000
Epoch 67: val_acc did not improve
67 0.0001
Epoch-68   20 batches   loss 0.4067 (0.4450)    accu 95.312 (89.453)
Epoch-68  15.3s Train: loss 0.4448      accu 89.6577    Valid: loss 0.4968      accu 85.3125
Epoch 68: val_acc did not improve
68 0.0001
Epoch-69   20 batches   loss 0.4216 (0.4409)    accu 92.188 (90.938)
Epoch-69  14.8s Train: loss 0.4419      accu 90.8482    Valid: loss 0.5001      accu 84.0625
Epoch 69: val_acc did not improve
69 0.0001
Epoch-70   20 batches   loss 0.4508 (0.4443)    accu 89.062 (90.078)
Epoch-70  15.5s Train: loss 0.4432      accu 90.1786    Valid: loss 0.4988      accu 85.6250
Epoch 70: val_acc did not improve
70 0.0001
Epoch-71   20 batches   loss 0.4376 (0.4401)    accu 92.188 (90.391)
Epoch-71  15.2s Train: loss 0.4383      accu 90.6250    Valid: loss 0.4953      accu 85.9375
Epoch 71: val_acc did not improve
71 0.0001
Epoch-72   20 batches   loss 0.4341 (0.4444)    accu 90.625 (91.016)
Epoch-72  15.6s Train: loss 0.4436      accu 90.9226    Valid: loss 0.5005      accu 84.0625
Epoch 72: val_acc did not improve
72 0.0001
Epoch-73   20 batches   loss 0.4302 (0.4443)    accu 93.750 (90.625)
Epoch-73  15.4s Train: loss 0.4436      accu 90.6250    Valid: loss 0.5032      accu 85.3125
Epoch 73: val_acc did not improve
73 0.0001
Epoch-74   20 batches   loss 0.4477 (0.4458)    accu 89.062 (90.391)
Epoch-74  15.6s Train: loss 0.4446      accu 90.4762    Valid: loss 0.4971      accu 86.2500
Epoch 74: val_acc did not improve
74 0.0001
Epoch-75   20 batches   loss 0.4290 (0.4371)    accu 90.625 (90.781)
Epoch-75  15.6s Train: loss 0.4379      accu 90.6994    Valid: loss 0.5010      accu 85.0000
Epoch 75: val_acc did not improve
75 0.0001
Epoch-76   20 batches   loss 0.4315 (0.4378)    accu 92.188 (91.719)
Epoch-76  15.1s Train: loss 0.4363      accu 91.8155    Valid: loss 0.5009      accu 85.0000
Epoch 76: val_acc did not improve
76 0.0001
Epoch-77   20 batches   loss 0.4294 (0.4349)    accu 92.188 (91.797)
Epoch-77  15.7s Train: loss 0.4348      accu 91.7411    Valid: loss 0.4961      accu 85.6250
Epoch 77: val_acc did not improve
77 0.0001
Epoch-78   20 batches   loss 0.4234 (0.4352)    accu 95.312 (90.859)
Epoch-78  15.2s Train: loss 0.4347      accu 90.8482    Valid: loss 0.5061      accu 84.3750
Epoch 78: val_acc did not improve
78 0.0001
Epoch-79   20 batches   loss 0.4333 (0.4343)    accu 93.750 (91.719)
Epoch-79  15.6s Train: loss 0.4341      accu 91.7411    Valid: loss 0.4974      accu 85.6250
Epoch 79: val_acc did not improve
79 0.0001
Epoch-80   20 batches   loss 0.4761 (0.4384)    accu 89.062 (90.938)
Epoch-80  15.1s Train: loss 0.4370      accu 91.1458    Valid: loss 0.5040      accu 85.3125
Epoch 80: val_acc did not improve
80 0.0001
Epoch-81   20 batches   loss 0.4682 (0.4355)    accu 90.625 (90.781)
Epoch-81  15.5s Train: loss 0.4351      accu 90.8482    Valid: loss 0.5104      accu 84.0625
Epoch 81: val_acc did not improve
81 0.0001
Epoch-82   20 batches   loss 0.4220 (0.4348)    accu 92.188 (91.641)
Epoch-82  15.5s Train: loss 0.4352      accu 91.5179    Valid: loss 0.4943      accu 86.2500
Epoch 82: val_acc did not improve
82 0.0001
Epoch-83   20 batches   loss 0.4228 (0.4275)    accu 92.188 (92.578)
Epoch-83  15.3s Train: loss 0.4295      accu 92.4107    Valid: loss 0.5019      accu 85.3125
Epoch 83: val_acc did not improve
83 0.0001
Epoch-84   20 batches   loss 0.4110 (0.4383)    accu 98.438 (91.484)
Epoch-84  15.0s Train: loss 0.4374      accu 91.3690    Valid: loss 0.5018      accu 84.6875
Epoch 84: val_acc did not improve
84 0.0001
Epoch-85   20 batches   loss 0.3963 (0.4322)    accu 98.438 (91.875)
Epoch-85  15.0s Train: loss 0.4324      accu 91.8155    Valid: loss 0.4933      accu 85.6250
Epoch 85: val_acc did not improve
85 0.0001
Epoch-86   20 batches   loss 0.3997 (0.4239)    accu 95.312 (92.500)
Epoch-86  15.4s Train: loss 0.4252      accu 92.3363    Valid: loss 0.5052      accu 85.6250
Epoch 86: val_acc did not improve
86 0.0001
Epoch-87   20 batches   loss 0.4192 (0.4312)    accu 89.062 (91.016)
Epoch-87  15.0s Train: loss 0.4294      accu 91.2202    Valid: loss 0.5046      accu 85.9375
Epoch 87: val_acc did not improve
87 0.0001
Epoch-88   20 batches   loss 0.3914 (0.4397)    accu 95.312 (90.938)
Epoch-88  15.9s Train: loss 0.4385      accu 91.1458    Valid: loss 0.4984      accu 85.6250
Epoch 88: val_acc did not improve
88 0.0001
Epoch-89   20 batches   loss 0.4692 (0.4327)    accu 84.375 (91.797)
Epoch-89  14.7s Train: loss 0.4308      accu 91.9643    Valid: loss 0.4904      accu 86.2500
Epoch 89: val_acc did not improve
89 0.0001
Epoch-90   20 batches   loss 0.4331 (0.4310)    accu 90.625 (91.484)
Epoch-90  15.5s Train: loss 0.4300      accu 91.5923    Valid: loss 0.4993      accu 85.0000
Epoch 90: val_acc did not improve
90 1e-05
Epoch-91   20 batches   loss 0.4412 (0.4288)    accu 90.625 (91.875)
Epoch-91  15.4s Train: loss 0.4268      accu 92.1131    Valid: loss 0.5014      accu 85.0000
Epoch 91: val_acc did not improve
91 1e-05
Epoch-92   20 batches   loss 0.4292 (0.4277)    accu 89.062 (91.797)
Epoch-92  15.9s Train: loss 0.4263      accu 91.8155    Valid: loss 0.5071      accu 84.3750
Epoch 92: val_acc did not improve
92 1e-05
Epoch-93   20 batches   loss 0.4261 (0.4221)    accu 93.750 (92.266)
Epoch-93  14.8s Train: loss 0.4227      accu 92.1875    Valid: loss 0.5022      accu 83.7500
Epoch 93: val_acc did not improve
93 1e-05
Epoch-94   20 batches   loss 0.4124 (0.4286)    accu 90.625 (92.188)
Epoch-94  15.6s Train: loss 0.4284      accu 92.2619    Valid: loss 0.5139      accu 84.0625
Epoch 94: val_acc did not improve
94 1e-05
Epoch-95   20 batches   loss 0.4253 (0.4275)    accu 93.750 (91.953)
Epoch-95  15.7s Train: loss 0.4273      accu 91.8899    Valid: loss 0.4902      accu 85.9375
Epoch 95: val_acc did not improve
95 1e-05
Epoch-96   20 batches   loss 0.4135 (0.4223)    accu 93.750 (92.969)
Epoch-96  14.6s Train: loss 0.4234      accu 92.9315    Valid: loss 0.4938      accu 87.1875
Epoch 96: val_acc did not improve
96 1e-05
Epoch-97   20 batches   loss 0.4323 (0.4232)    accu 92.188 (92.578)
Epoch-97  14.5s Train: loss 0.4229      accu 92.5595    Valid: loss 0.4963      accu 86.2500
Epoch 97: val_acc did not improve
97 1e-05
Epoch-98   20 batches   loss 0.4485 (0.4210)    accu 90.625 (92.734)
Epoch-98  15.8s Train: loss 0.4199      accu 92.7827    Valid: loss 0.4970      accu 85.6250
Epoch 98: val_acc did not improve
98 1e-05
Epoch-99   20 batches   loss 0.4330 (0.4207)    accu 89.062 (92.109)
Epoch-99  15.5s Train: loss 0.4221      accu 91.6667    Valid: loss 0.5075      accu 84.3750
Epoch 99: val_acc did not improve
99 1e-05
Epoch-100  20 batches   loss 0.4174 (0.4234)    accu 93.750 (92.656)
Epoch-100 15.0s Train: loss 0.4247      accu 92.3363    Valid: loss 0.4951      accu 85.6250
Epoch 100: val_acc did not improve
100 1e-05
Epoch-101  20 batches   loss 0.4230 (0.4208)    accu 92.188 (92.734)
Epoch-101 15.8s Train: loss 0.4231      accu 92.4851    Valid: loss 0.4998      accu 86.8750
Epoch 101: val_acc did not improve
101 1e-05
Epoch-102  20 batches   loss 0.3941 (0.4198)    accu 95.312 (93.594)
Epoch-102 15.7s Train: loss 0.4201      accu 93.4524    Valid: loss 0.5029      accu 84.3750
Epoch 102: val_acc did not improve
102 1e-05
Epoch-103  20 batches   loss 0.4358 (0.4206)    accu 92.188 (92.344)
Epoch-103 15.2s Train: loss 0.4197      accu 92.5595    Valid: loss 0.4966      accu 85.6250
Epoch 103: val_acc did not improve
103 1e-05
Epoch-104  20 batches   loss 0.5177 (0.4238)    accu 81.250 (92.656)
Epoch-104 15.2s Train: loss 0.4226      accu 92.6339    Valid: loss 0.4968      accu 85.9375
Epoch 104: val_acc did not improve
104 1e-05
Epoch-105  20 batches   loss 0.4262 (0.4229)    accu 95.312 (92.656)
Epoch-105 15.4s Train: loss 0.4240      accu 92.6339    Valid: loss 0.5072      accu 84.3750
Epoch 105: val_acc did not improve
105 1e-05
Epoch-106  20 batches   loss 0.4147 (0.4227)    accu 92.188 (92.266)
Epoch-106 15.1s Train: loss 0.4233      accu 92.1131    Valid: loss 0.4968      accu 84.6875
Epoch 106: val_acc did not improve
106 1e-05
Epoch-107  20 batches   loss 0.4313 (0.4298)    accu 92.188 (91.484)
Epoch-107 15.4s Train: loss 0.4293      accu 91.5923    Valid: loss 0.5009      accu 84.3750
Epoch 107: val_acc did not improve
107 1e-05
Epoch-108  20 batches   loss 0.4068 (0.4252)    accu 96.875 (92.422)
Epoch-108 15.1s Train: loss 0.4252      accu 92.4851    Valid: loss 0.4991      accu 86.2500
Epoch 108: val_acc did not improve
108 1e-05
Epoch-109  20 batches   loss 0.3901 (0.4182)    accu 98.438 (93.438)
Epoch-109 15.0s Train: loss 0.4176      accu 93.3780    Valid: loss 0.5081      accu 84.0625
Epoch 109: val_acc did not improve
109 1e-05
Epoch-110  20 batches   loss 0.4278 (0.4224)    accu 92.188 (92.578)
Epoch-110 15.1s Train: loss 0.4225      accu 92.7083    Valid: loss 0.5033      accu 84.6875
Epoch 110: val_acc did not improve
110 1.0000000000000002e-06
Epoch-111  20 batches   loss 0.4435 (0.4308)    accu 92.188 (91.953)
Epoch-111 15.7s Train: loss 0.4301      accu 92.0387    Valid: loss 0.4958      accu 85.6250
Epoch 111: val_acc did not improve
111 1.0000000000000002e-06
Epoch-112  20 batches   loss 0.4916 (0.4277)    accu 82.812 (91.719)
Epoch-112 15.2s Train: loss 0.4259      accu 92.0387    Valid: loss 0.4972      accu 85.9375
Epoch 112: val_acc did not improve
112 1.0000000000000002e-06
Epoch-113  20 batches   loss 0.4386 (0.4245)    accu 90.625 (92.266)
Epoch-113 15.8s Train: loss 0.4235      accu 92.4107    Valid: loss 0.4968      accu 83.7500
Epoch 113: val_acc did not improve
113 1.0000000000000002e-06
Epoch-114  20 batches   loss 0.3824 (0.4213)    accu 98.438 (92.500)
Epoch-114 14.6s Train: loss 0.4237      accu 92.3363    Valid: loss 0.4991      accu 84.3750
Epoch 114: val_acc did not improve
114 1.0000000000000002e-06
Epoch-115  20 batches   loss 0.4038 (0.4248)    accu 93.750 (92.578)
Epoch-115 15.0s Train: loss 0.4237      accu 92.7083    Valid: loss 0.5053      accu 84.3750
Epoch 115: val_acc did not improve
115 1.0000000000000002e-06
Epoch-116  20 batches   loss 0.4158 (0.4243)    accu 93.750 (92.812)
Epoch-116 15.4s Train: loss 0.4259      accu 92.5595    Valid: loss 0.4988      accu 85.3125
Epoch 116: val_acc did not improve
116 1.0000000000000002e-06
Epoch-117  20 batches   loss 0.4553 (0.4219)    accu 89.062 (93.047)
Epoch-117 15.4s Train: loss 0.4214      accu 93.0804    Valid: loss 0.5006      accu 85.9375
Epoch 117: val_acc did not improve
117 1.0000000000000002e-06
Epoch-118  20 batches   loss 0.4303 (0.4257)    accu 92.188 (92.188)
Epoch-118 15.6s Train: loss 0.4282      accu 91.9643    Valid: loss 0.4968      accu 86.2500
Epoch 118: val_acc did not improve
118 1.0000000000000002e-06
Epoch-119  20 batches   loss 0.4469 (0.4200)    accu 90.625 (92.578)
Epoch-119 14.8s Train: loss 0.4198      accu 92.6339    Valid: loss 0.5011      accu 84.3750
Epoch 119: val_acc did not improve
119 1.0000000000000002e-06
Epoch-120  20 batches   loss 0.4366 (0.4276)    accu 89.062 (92.188)
Epoch-120 15.1s Train: loss 0.4284      accu 92.1131    Valid: loss 0.5004      accu 84.6875
Epoch 120: val_acc did not improve
Best val_acc: 87.1875 from epoch-59
Save train and validation log into into ./results/Football/SGN/0_log.csv
/home/louiemiddle/SGN/main.py:208: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(checkpoint)['state_dict'])
Test: accuracy 86.875, time: 6.77s
(base) louiemiddle@HEI-WINalNumber:~/SGN$ 